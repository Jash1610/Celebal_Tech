1. Load NYC Taxi Data to DataLake/Blob_Storage/DataBricks and Extract Data
Assume you have the NYC Taxi data in a CSV format stored in a DataLake/Blob_Storage, and you have mounted this storage in Databricks. Here's how you can load the data:

python
Copy code
# Load the data into a DataFrame
df = spark.read.csv("dbfs:/mnt/your_mount_path/nyc_taxi_data.csv", header=True, inferSchema=True)
df.show()


2. Perform Queries Using PySpark
Query 1: Add a Column Named "Revenue" into DataFrame
python
Copy code
from pyspark.sql.functions import col, sum as _sum

# Add "Revenue" column
df = df.withColumn("Revenue", 
                   col("Fare_amount") + col("Extra") + col("MTA_tax") + 
                   col("Improvement_surcharge") + col("Tip_amount") + 
                   col("Tolls_amount") + col("Total_amount"))

df.show()


Query 2: Increasing Count of Total Passengers in New York City by Area
Assuming "pickup_area" is a column in the dataset indicating the area:

python
Copy code
# Count total passengers by area
passenger_count_by_area = df.groupBy("pickup_area").agg(_sum("passenger_count").alias("total_passengers")).orderBy("total_passengers", ascending=False)
passenger_count_by_area.show()


Query 3: Realtime Average Fare/Total Earning Amount Earned by 2 Vendors
Assuming "VendorID" is a column indicating the vendor:

python
Copy code
# Average fare/total earning by vendors
average_earning_by_vendor = df.groupBy("VendorID").agg(
    _sum("Revenue").alias("total_earning"),
    _sum("Revenue") / _sum("VendorID").alias("average_earning")
)
average_earning_by_vendor.show()


Query 4: Moving Count of Payments Made by Each Payment Mode
Assuming "payment_type" is a column indicating the payment mode:

python
Copy code
# Count of payments by payment mode
payment_count_by_mode = df.groupBy("payment_type").count().orderBy("count", ascending=False)
payment_count_by_mode.show()


Query 5: Highest Two Gaining Vendors on a Particular Date with Number of Passengers and Total Distance by Cab
Assuming "trip_date" is a column indicating the date:

python
Copy code
from pyspark.sql.functions import desc

# Filter by a particular date
date = "YYYY-MM-DD"
top_vendors = df.filter(col("trip_date") == date) \
                .groupBy("VendorID") \
                .agg(_sum("Revenue").alias("total_revenue"), 
                     _sum("passenger_count").alias("total_passengers"), 
                     _sum("trip_distance").alias("total_distance")) \
                .orderBy(desc("total_revenue")) \
                .limit(2)
top_vendors.show()


Query 6: Most Number of Passengers Between a Route of Two Locations
Assuming "pickup_location" and "dropoff_location" are columns indicating the route:

python
Copy code
# Most number of passengers between routes
passenger_count_by_route = df.groupBy("pickup_location", "dropoff_location") \
                             .agg(_sum("passenger_count").alias("total_passengers")) \
                             .orderBy("total_passengers", ascending=False)
passenger_count_by_route.show()


Query 7: Get Top Pickup Locations with Most Passengers in Last 5/10 Seconds
Assuming "pickup_datetime" is a column indicating the pickup time:

python
Copy code
from pyspark.sql.functions import current_timestamp

# Filter last 5 seconds
time_window = 5  # seconds
recent_pickups = df.filter((current_timestamp() - col("pickup_datetime")).cast("long") <= time_window) \
                   .groupBy("pickup_location") \
                   .agg(_sum("passenger_count").alias("total_passengers")) \
                   .orderBy("total_passengers", ascending=False)
recent_pickups.show()
